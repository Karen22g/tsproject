{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dfd47ad-0fd6-4f32-8dff-22f24a6298a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from joblib import dump, load\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import re\n",
    "from keras.layers import LSTM\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.stats.stattools import jarque_bera\n",
    "import statsmodels.api as sm\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6264c3d-a835-4924-b71c-d2425c299ed1",
   "metadata": {},
   "source": [
    "# Modelos de redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02918c1f-3794-4af8-a9d2-eb7a045e14c7",
   "metadata": {},
   "source": [
    "Apliquemos a continuación Modelos de Redes Neuronales a nuestra serie de tiempo, para conocer el comportamiento del pronóstico que se puede alcanzar a partir de estos. En este apartado estaremos aplicando:\n",
    "\n",
    "1. Recurrent neural networks\n",
    "2. Multilayer-perceptron\n",
    "3. Long-short term memory\n",
    "\n",
    "Durante el análisis se estarán construyendo modelos para distintas combinaciones de input layers, neuronas y tasas de drop-out.\n",
    "\n",
    "Cada una de los apartados a continuación incluyen funciones de creación de la red neuronal, además se diseñó una función que retorna el mejor modelo en cada caso, para automatizar la creación y selección para cada combinación. Carguemos los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ef7e906-1914-4bdf-b3a3-f5afd7d81aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel(\"loadsts.xlsx\")\n",
    "df.index = df['Posted_date']\n",
    "df.index.freq = 'D'\n",
    "ts = df['Loads']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da7e178-2458-4b7b-abe8-b7087c1d0a8d",
   "metadata": {},
   "source": [
    "En el caso de red neuronales, escalaremos los datos con el fin de mejorar el tiempo de alcance de la convergencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b077e83-453b-4423-b1b1-6f67ca0fbb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loads_escaler'] = df['Loads']\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "ts_esc = scaler.fit_transform(np.array(df['loads_escaler']).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8a7aca-d2ec-4585-b75a-e43ff67a7793",
   "metadata": {},
   "source": [
    "El conjunto de entrenamiento tendrá el 80% datos en este entrenamiento de red neuronal. Guardaremos los conjuntos con los valores reales y los valores escalados a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5887a93-1568-4df9-bdd2-4e201fb2cd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = split(ts, 0.8)\n",
    "train_esc, val_esc, test_esc = split(ts_esc, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f68ec-da34-4b8a-862c-10101ca5c456",
   "metadata": {},
   "source": [
    "## 1. Funciones complementarias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb0caae-bc1a-4814-bfa7-eaa03101821d",
   "metadata": {},
   "source": [
    "Al igual que en los otros capítulos estaremos usando las funciones de apoyo para separación de la serie, evaluación de supuestos y evaluación de los 'score' de pronóstico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d7ca9b8-b1de-4014-a9e4-4f5beb6d721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(serie, trainportion):\n",
    "    perct = math.ceil(len(ts) * trainportion)\n",
    "    perc2 = math.ceil(len(ts)*((1 - trainportion)/2))\n",
    "    train = serie[0:perct]\n",
    "    validation = serie[perct:perct+perc2]\n",
    "    test = serie[perct+perc2:]\n",
    "    return train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42cb67c6-5af4-4aa7-8ce8-4705f70cb4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculo de los errores\n",
    "def errors(y_pred, y_real):\n",
    "    \n",
    "    mae = mean_absolute_error(y_real, y_pred)\n",
    "    mape = 100*(sum(abs(((y_real - y_pred)/y_real)))/len(y_real))\n",
    "    mse = mean_squared_error(y_real , y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_real, y_pred)\n",
    "    \n",
    "    return mae, mape, mse, rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0640871a-db29-4cf2-9c50-787b1f8a6177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluacion de supuestos\n",
    "def assumptions(residuals):\n",
    "\n",
    "    warning = \"\"\n",
    "    a = 0\n",
    "    \n",
    "    #Independency\n",
    "    #H0: The residuals are independent.\n",
    "    #H1: The residuals aren't independent.    \n",
    "    ljung_box_test = acorr_ljungbox(residuals, lags=10, return_df=True)\n",
    "    ljung_box_pvalue = ljung_box_test['lb_pvalue'].values[0]\n",
    "    if ljung_box_pvalue < 0.05:\n",
    "        warning= warning + \" The residuals aren't independent\"\n",
    "        a=a+1\n",
    "\n",
    "    #Normality test\n",
    "    #H0: The residuals have a normal distribution.\n",
    "    #H1: The residuals don't have a normal distribution.\n",
    "    jb_stat, jb_pvalue, skewness, kurtosis = jarque_bera(residuals)\n",
    "    if jb_pvalue < 0.05:\n",
    "        a = a+1\n",
    "        warning = warning + \" The residuals do not follow a normal distribution\"\n",
    "\n",
    "    #homocedasticity\n",
    "    #H0: The residuals are homocedastic.\n",
    "    #H1: The residuals aren't homocedastic.\n",
    "    arch_stat, arch_pvalue, _, _ = het_arch(residuals)\n",
    "    if arch_pvalue < 0.05:\n",
    "        warning= warning + \" The residuals aren't homocedastic\"\n",
    "        a=a+1\n",
    "\n",
    "    return arch_pvalue, jb_pvalue, ljung_box_pvalue, warning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede2db68-ceac-491a-a48c-edbc8631ef7c",
   "metadata": {},
   "source": [
    "Además, agregaremos la función _makeXy_ que se encarga de organizar la serie de tiempo en un arreglo matricial que genera el input layer de $nbtimesteps$ y la predicción $y$ de un valor a la vez. Durante la selección del mejor modelo de cada tipo de red neuronal estaremos probando esta división para los valores de 3, 4, 5 días para entrenar la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51dec72a-5ab7-41c9-bb7e-e9d99a4af9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeXy(ts, nbtimesteps):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(nb_timesteps, ts.shape[0]):\n",
    "        X.append(list(ts[i-nbtimesteps:i-1])) #Regressors\n",
    "        y.append(ts[i]) #Target\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb984047-6808-4dc3-a900-f7509dda494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#yo no quiero pronosticar un valor a la vez\n",
    "#nbtimesteps va a cambiar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34c5608-28db-4bb9-9937-31bc17c75eb0",
   "metadata": {},
   "source": [
    "## 1. Recurrent neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f3a208-b76e-44cf-ad31-7a012d1730e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explicar como funcion tanh o ver cual se escoge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6822821f-2b5c-4d5c-acb3-bcfbe29e58a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#una sola capa\n",
    "#neuronas cambia\n",
    "#dropout rate cambia\n",
    "#funcion de activación tanh\n",
    "#input shape cambia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5f4d9513-7e77-44f5-a5c4-8fe3f95f7c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN(neurons, input_shape, activation,dropout_rate): #Creación RNN\n",
    "    model_RNN = Sequential()\n",
    "    # Capa SimpleRNN con 10 neuronas\n",
    "    model_RNN.add(SimpleRNN(neurons, input_shape=input_shape, \n",
    "                        activation=activation, return_sequences=False))\n",
    "    model_RNN.add(Dropout(dropout_rate))\n",
    "    model_RNN.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    return model_RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2328eef4-2ea0-42b3-b5f3-df6b860b3b03",
   "metadata": {},
   "source": [
    "## 2. Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee7a5cd9-7727-43bf-88d9-834b864a43e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crear el modelo\n",
    "def create_mlp(neurons, dropout_rate): #agregar como input shape\n",
    "    \n",
    "    input_layer = Input(shape=(7,), dtype='float32')\n",
    "    dense_layer = Dense(neurons, activation='tanh')(input_layer)\n",
    "    dropout_layer = Dropout(dropout_rate)(dense_layer)\n",
    "    output_layer = Dense(1, activation='linear')(dropout_layer)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9142159-08e0-4468-a77d-db7a0213fd5f",
   "metadata": {},
   "source": [
    "## 3. Long Short Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d1e3a-8956-4b23-9e7f-884493ade00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crear el modelo\n",
    "def create_lstm(neurons, dropout_rate):\n",
    "    \n",
    "    input_layer = Input(shape=(7,1), dtype='float32')\n",
    "    lstm_layer1 = LSTM(neurons, input_shape=(7,1), return_sequences=True)(input_layer)\n",
    "    dropout_layer = Dropout(drop_out)(lstm_layer2)\n",
    "    output_layer = Dense(1, activation='linear')(dropout_layer)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317a02ef-747d-443b-8a53-3e1fcf7876cc",
   "metadata": {},
   "source": [
    "## 4. Seleccion mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bdf7f690-353c-4501-80cd-52d1b2180e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleccionar el mejor modelo\n",
    "# aca puede tocar cambiar las carpetas\n",
    "def select_mlp(model, batch_size,X_train, y_train, X_val, y_val):\n",
    "\n",
    "    save_weights_at = os.path.join('keras_models', f'{model.name}_weights_batch{batch_size}_{{epoch:02d}}-{{val_loss:.4f}}.keras')\n",
    "    save_best = ModelCheckpoint(save_weights_at, monitor='val_loss', verbose=0,\n",
    "                            save_best_only=True, save_weights_only=False, mode='min', save_freq='epoch');\n",
    "    \n",
    "    history_filename = os.path.join('historynn', f'history_{model.name}_batch{batch_size}.joblib')\n",
    "    history_airp = None\n",
    "\n",
    "    if os.path.exists(history_filename):\n",
    "        history_filename = history_filename\n",
    "        print(\"El archivo '{history_filename}' ya existe. Se ha cargado el historial del entrenamiento.\")\n",
    "        \n",
    "    else:\n",
    "        history_airp = model.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=10,\n",
    "                 verbose=2, callbacks=[save_best], validation_data=(X_val, y_val),\n",
    "                 shuffle=True);\n",
    "        dump(history_airp.history, history_filename)\n",
    "        print(\"El entrenamiento se ha completado y el historial ha sido guardado en '{history_filename}'\")\n",
    "\n",
    "    model_dir = 'keras_models'\n",
    "    files = os.listdir(model_dir)\n",
    "    pattern = rf\"{re.escape(model.name)}_weights_batch{batch_size}_(\\d+)-([\\d\\.]+)\\.keras\"\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_file = None\n",
    "    best_model = None\n",
    "    \n",
    "    for file in files:\n",
    "        match = re.match(pattern, file)\n",
    "        if match:\n",
    "            epoch = int(match.group(1))\n",
    "            val_loss = float(match.group(2))\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_file = file\n",
    "\n",
    "    if best_model_file:\n",
    "        best_model_path = os.path.join(model_dir, best_model_file)\n",
    "        print(f\"Cargando el mejor modelo: {best_model_file} con val_loss: {best_val_loss}\")\n",
    "        best_model = load_model(best_model_path)\n",
    "    else:\n",
    "        print(\"No se encontraron archivos de modelos que coincidan con el patrón.\")\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a559cabb-9d16-4835-8c54-8b9a39a4efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecast\n",
    "def forecast_nw(best_model, X_val):\n",
    "    preds = best_model.predict(X_val)\n",
    "    preds = scaler.inverse_transform(preds)\n",
    "    preds = np.squeeze(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156cc0a2-cc6b-4c72-b7e8-a9d56b517270",
   "metadata": {},
   "source": [
    "## 5. Implementación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d382a78-dde5-4a24-b80f-e91bdb298edd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando: Neuronas: 10, Dropout: 0.2, Batchsize: 16\n",
      "Epoch 1/10\n",
      "56/56 - 5s - 97ms/step - loss: 0.3895 - val_loss: 0.3581\n",
      "Epoch 2/10\n",
      "56/56 - 1s - 19ms/step - loss: 0.1277 - val_loss: 0.1940\n",
      "Epoch 3/10\n",
      "56/56 - 1s - 14ms/step - loss: 0.0946 - val_loss: 0.1535\n",
      "Epoch 4/10\n",
      "56/56 - 1s - 15ms/step - loss: 0.0824 - val_loss: 0.1238\n",
      "Epoch 5/10\n",
      "56/56 - 1s - 11ms/step - loss: 0.0666 - val_loss: 0.1024\n",
      "Epoch 6/10\n",
      "56/56 - 1s - 13ms/step - loss: 0.0562 - val_loss: 0.0911\n",
      "Epoch 7/10\n",
      "56/56 - 1s - 9ms/step - loss: 0.0477 - val_loss: 0.0714\n",
      "Epoch 8/10\n",
      "56/56 - 1s - 12ms/step - loss: 0.0435 - val_loss: 0.0609\n",
      "Epoch 9/10\n",
      "56/56 - 1s - 11ms/step - loss: 0.0354 - val_loss: 0.0491\n",
      "Epoch 10/10\n",
      "56/56 - 1s - 15ms/step - loss: 0.0309 - val_loss: 0.0486\n",
      "El entrenamiento se ha completado y el historial ha sido guardado en '{history_filename}'\n",
      "Cargando el mejor modelo: functional_4_weights_batch16_10-0.0486.keras con val_loss: 0.0486\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#pronosticar validación\u001b[39;00m\n\u001b[0;32m     24\u001b[0m val_pred \u001b[38;5;241m=\u001b[39m forecast_nw(best_model, X_val)\n\u001b[1;32m---> 25\u001b[0m mae_v, mape_v, mse_v, rmse_v, r2_v \u001b[38;5;241m=\u001b[39m errors(val_pred, \u001b[43mdf_val\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m7\u001b[39m:])\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#calculo de residuos residuals = df_val['Price'].reset_index(drop=True).loc[7:]-pred_PRES\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#con mejor modelo se revisan supuestos del error en entrenamiento\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#con mejor modelo se revisan supuestos del error en test\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m#Se guardan los resultados\u001b[39;00m\n\u001b[0;32m     38\u001b[0m val_result\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlp\u001b[39m\u001b[38;5;124m\"\u001b[39m, dropout_rate, neurons, batch_size, mape_v, mae_v, mse_v, rmse_v, r2_v]) \u001b[38;5;66;03m#jb_pvalue_m, ljung_box_pvalue_m])\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_val' is not defined"
     ]
    }
   ],
   "source": [
    "#probar los distintos modelos\n",
    "dropout_rates = [0.2, 0.4]#, 0.6, 0.8]\n",
    "neurons_list = [10, 100]#, 1000, 10000]\n",
    "batch_sizes = [16, 32]#, 64, 128]\n",
    "\n",
    "val_result = []\n",
    "test_result = []\n",
    "\n",
    "#probar todas las combinaciones\n",
    "for dropout_rate in dropout_rates:\n",
    "    for neurons in neurons_list:\n",
    "        for batch_size in batch_sizes:\n",
    "            \n",
    "            print(f\"Evaluando: Neuronas: {neurons}, Dropout: {dropout_rate}, Batchsize: {batch_size}\")\n",
    "            model = create_mlp(neurons, dropout_rate)\n",
    "\n",
    "            X_train, y_train = makeXy(train_esc, 8)\n",
    "            X_val, y_val = makeXy(val_esc,8) #makeXy(ts, 5) #probar para 3, 4, 5\n",
    "\n",
    "            #seleccionar el mejor modelo\n",
    "            best_model = select_mlp(model, batch_size,X_train, y_train, X_val, y_val)\n",
    "\n",
    "            #pronosticar validación\n",
    "            val_pred = forecast_nw(best_model, X_val)\n",
    "            mae_v, mape_v, mse_v, rmse_v, r2_v = errors(val_pred, df_val['Price'].reset_index(drop=True).loc[7:])\n",
    "\n",
    "            #calculo de residuos residuals = df_val['Price'].reset_index(drop=True).loc[7:]-pred_PRES\n",
    "            #con mejor modelo se revisan supuestos del error en entrenamiento\n",
    "            #con mejor modelo se revisan supuestos del error en test\n",
    "            \n",
    "            #con mejor modelo de esa combinación se predice test\n",
    "            \n",
    "            #pronosticar test\n",
    "            #test_pred = forecast_nw(best_model, X_test)\n",
    "            #mae_v, mape_v, mse_v, rmse_v, r2_v = errors(test_pred, df_test['Price'].reset_index(drop=True).loc[7:])\n",
    "\n",
    "            #Se guardan los resultados\n",
    "            #val_result.append(['price', \"mlp\", dropout_rate, neurons, batch_size, mape_v, mae_v, mse_v, rmse_v, r2_v]) #jb_pvalue_m, ljung_box_pvalue_m])\n",
    "            #test_result.append(['price', \"mlp\", dropout_rate, neurons, batch_size, mape_t, mae_t, mse_t, rmse_t, r2_t, ljung_box_pvalue_t])\n",
    "             \n",
    "#dfval = pd.DataFrame(val_result, columns=['variable','model', 'val_size', 'MAPE', 'MAE', 'MSE', 'RMSE', 'R2', 'jarque-bera_p','ljungbox_p'])\n",
    "#dftest = pd.DataFrame(test_result, columns=['variable','model', 'test_size', 'MAPE', 'MAE', 'MSE', 'RMSE', 'R2', 'ljungbox_p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eafee0-a42a-4f20-aa12-f5ef04f9a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5.5, 5.5))\n",
    "plt.plot(range(50), df_val['Price'].reset_index(drop=True).loc[7:56], linestyle='-', marker='*', color='r')\n",
    "plt.plot(range(50), pred_PRES[:50], linestyle='-', marker='.', color='b')\n",
    "plt.legend(['Actual','Predicted'], loc=2)\n",
    "plt.title('Actual vs Predicted price')\n",
    "plt.ylabel('Air Pressure')\n",
    "plt.xlabel('Index');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c377ef8-cbe5-4df9-87fb-22a14ab59f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## organizar ciclo\n",
    "## seleccionar mejor modelo por red neuronal hacer función default con carpetas que dependen del tipo de red\n",
    "## guardar resultados validación y testing\n",
    "## hacer modelo mejorado\n",
    "## análisis de resultados\n",
    "# subir jupyter book"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
